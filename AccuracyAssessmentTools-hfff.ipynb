{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/alemlakes/accuracyAssessmentTools-hfff/blob/main/AccuracyAssessmentTools-hfff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Assessment Tools Notebook\n",
    "\n",
    "## Purpose\n",
    "This notebook demonstrates how to run map-accuracy assessments using the `acc_assessment` package, with examples for **GUE**, **MCEM**, **Stehman**, **Olofsson**, and **Congalton**.\n",
    "\n",
    "It is designed to help you:\n",
    "- Load map/reference datasets\n",
    "- Validate basic data quality and map/reference alignment\n",
    "- Compute and compare accuracy/area estimates across methods\n",
    "- Understand differences between probabilistic and crisp workflows\n",
    "\n",
    "## How to use this notebook\n",
    "1. Run the setup and import sections first.\n",
    "2. (Optional) Run the test section to confirm the environment is healthy.\n",
    "3. Run the integrated four-method workflow for the main end-to-end example. Note that if you use your own data, update the input file paths in the integrated workflow and rerun from the shared-input loading step onward.\n",
    "4. Then run any method-specific example sections below, as needed.\n",
    "\n",
    "## Expected inputs (integrated example)\n",
    "- A map probability table with `id`, `strata`, and class-probability columns\n",
    "- A reference probability table with matching `id` rows\n",
    "- A strata population table with `strata` and `population` columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9E5EFNO6Dzfx"
   },
   "source": [
    "# Installs and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogwaLH3BC4xH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One-time setup / refresh for Colab\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "repo_url = \"https://github.com/alemlakes/accuracyAssessmentTools-hfff.git\"\n",
    "repo_dir = \"/content/accuracyAssessmentTools-hfff\"\n",
    "\n",
    "os.makedirs(\"/content\", exist_ok=True)\n",
    "\n",
    "if not os.path.exists(repo_dir):\n",
    "    print(\"Cloning repository...\")\n",
    "    subprocess.run([\"git\", \"clone\", repo_url, repo_dir], check=True)\n",
    "else:\n",
    "    print(\"Repository exists; pulling latest changes...\")\n",
    "    subprocess.run([\"git\", \"-C\", repo_dir, \"pull\", \"--ff-only\"], check=False)\n",
    "\n",
    "os.chdir(repo_dir)\n",
    "print(\"Installing package...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \".\"], check=True)\n",
    "\n",
    "print(\"Setup complete:\", repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrLA0qviG2v2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from acc_assessment.olofsson import Olofsson\n",
    "from acc_assessment.congalton import Congalton\n",
    "from acc_assessment.stehman import Stehman\n",
    "from acc_assessment.cardille import Cardille\n",
    "from acc_assessment.gue import GUE\n",
    "from acc_assessment.mcem import MCEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tASR5FX-GITX"
   },
   "source": [
    "# Verify that the code works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtKKZboXGHcM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "# Integrated Four-Method Example (Same Data)\n",
    "\n",
    "This section uses one shared probabilistic dataset for all four methods.\n",
    "\n",
    "Workflow:\n",
    "1. **Sets** three input file paths (map probabilities, reference probabilities, and strata populations).\n",
    "2. **Loads** the three files and builds shared inputs.\n",
    "3. **Runs** **GUE** (analytical) and **MCEM** (simulation) directly on those probabilistic inputs.\n",
    "4. **Converts** those same probabilities to crisp classes using `argmax`.\n",
    "5. **Runs** **Stehman** and **Olofsson** on the hardened table.\n",
    "\n",
    "Note: each code cell prints labeled outputs so you can see what object or result is being shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Set the three input files used by the integrated four-method example\n",
    "map_file_same = \"./tests/map_data_table.csv\"\n",
    "ref_file_same = \"./tests/ref_data_table.csv\"\n",
    "strata_file_same = \"./tests/strata_population_table.csv\"\n",
    "\n",
    "print(\"Map file:\", map_file_same)\n",
    "print(\"Reference file:\", ref_file_same)\n",
    "print(\"Strata population file:\", strata_file_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the three files and derive shared variables for all four approaches\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "prob_map_same = pd.read_csv(map_file_same)\n",
    "prob_ref_same = pd.read_csv(ref_file_same)\n",
    "strata_population_df = pd.read_csv(strata_file_same)\n",
    "\n",
    "# Early alignment checks: map and reference tables must match by id\n",
    "for table_name, table_df in [(\"map\", prob_map_same), (\"reference\", prob_ref_same)]:\n",
    "    if \"id\" not in table_df.columns:\n",
    "        raise ValueError(f\"{table_name} table is missing required column: 'id'\")\n",
    "\n",
    "if len(prob_map_same) != len(prob_ref_same):\n",
    "    raise ValueError(\n",
    "        f\"Row count mismatch: map has {len(prob_map_same)} rows, \"\n",
    "        f\"reference has {len(prob_ref_same)} rows.\"\n",
    "    )\n",
    "\n",
    "map_ids = prob_map_same[\"id\"].reset_index(drop=True)\n",
    "ref_ids = prob_ref_same[\"id\"].reset_index(drop=True)\n",
    "\n",
    "if not map_ids.equals(ref_ids):\n",
    "    mismatch_rows = map_ids.ne(ref_ids)\n",
    "    first_bad_row = int(mismatch_rows.idxmax())\n",
    "    raise ValueError(\n",
    "        \"ID order mismatch between map and reference tables. \"\n",
    "        f\"First mismatch at row position {first_bad_row}: \"\n",
    "        f\"map id={map_ids.iloc[first_bad_row]!r}, \"\n",
    "        f\"reference id={ref_ids.iloc[first_bad_row]!r}.\"\n",
    "    )\n",
    "\n",
    "strata_population_same = dict(\n",
    "    zip(strata_population_df[\"strata\"], strata_population_df[\"population\"])\n",
    ")\n",
    "class_cols_same = [c for c in prob_map_same.columns if c not in [\"strata\", \"id\"]]\n",
    "target_class_same = class_cols_same[0]\n",
    "N_same = sum(strata_population_same.values())\n",
    "\n",
    "print(\"Map/reference alignment check: PASS (same row count and id order)\")\n",
    "print(\"Strata population table from strata_file_same\")\n",
    "display(strata_population_df.set_index(\"strata\"))\n",
    "\n",
    "print(\"\\nPreview: prob_map_same (map class probabilities, first 5 rows)\")\n",
    "display(HTML(prob_map_same.head().to_html(index=False)))\n",
    "\n",
    "print(\"\\nPreview: prob_ref_same (reference class probabilities, first 5 rows)\")\n",
    "display(HTML(prob_ref_same.head().to_html(index=False)))\n",
    "\n",
    "print(\"\\nPreview: map and reference probabilities side by side (first 5 rows)\")\n",
    "side_by_side_preview = pd.concat(\n",
    "    [\n",
    "        prob_map_same.head().add_prefix(\"map_\"),\n",
    "        prob_ref_same.head().add_prefix(\"ref_\"),\n",
    "    ],\n",
    "    axis=1,\n",
    "    )\n",
    "display(HTML(side_by_side_preview.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate shared probabilistic input tables (row counts + row-level data quality checks)\n",
    "def probability_table_report(df, table_name, class_cols, id_col=\"id\", tolerance=1e-9):\n",
    "    print(f\"\\n=== Data quality report for file: {table_name} ===\")\n",
    "    print(f\"Rows: {len(df)}\")\n",
    "    print(f\"Class columns: {class_cols}\")\n",
    "\n",
    "    if len(class_cols) == 0:\n",
    "        print(\"No class probability columns found. Skipping checks.\")\n",
    "        return\n",
    "\n",
    "    numeric_probs = df[class_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    has_missing = numeric_probs.isna().any(axis=1)\n",
    "    has_negative = (numeric_probs < -tolerance).any(axis=1)\n",
    "    has_gt_one = (numeric_probs > (1 + tolerance)).any(axis=1)\n",
    "    row_sum = numeric_probs.sum(axis=1)\n",
    "    sum_not_one = ~row_sum.between(1 - tolerance, 1 + tolerance)\n",
    "\n",
    "    max_per_row = numeric_probs.max(axis=1)\n",
    "    n_tied_max = numeric_probs.eq(max_per_row, axis=0).sum(axis=1)\n",
    "    max_not_unique = n_tied_max > 1\n",
    "\n",
    "    tied_max_classes = numeric_probs.apply(\n",
    "        lambda row: \", \".join(\n",
    "            [\n",
    "                col\n",
    "                for col in class_cols\n",
    "                if abs(float(row[col]) - float(row.max())) <= tolerance\n",
    "            ]\n",
    "        ) if not row.isna().any() else \"\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    if id_col in df.columns:\n",
    "        duplicated_id = df[id_col].duplicated(keep=False)\n",
    "        row_id = df[id_col]\n",
    "    else:\n",
    "        duplicated_id = pd.Series([False] * len(df), index=df.index)\n",
    "        row_id = df.index\n",
    "\n",
    "    issue_mask = (\n",
    "        has_missing\n",
    "        | has_negative\n",
    "        | has_gt_one\n",
    "        | sum_not_one\n",
    "        | duplicated_id\n",
    "        | max_not_unique\n",
    "    )\n",
    "\n",
    "    issue_report = pd.DataFrame({\n",
    "        \"row position\": df.index,\n",
    "        \"id\": row_id,\n",
    "        \"row sum\": row_sum.round(6),\n",
    "        \"missing or non-numeric\": has_missing,\n",
    "        \"negative probability\": has_negative,\n",
    "        \"probability gt 1\": has_gt_one,\n",
    "        \"row sum not 1\": sum_not_one,\n",
    "        \"max not unique\": max_not_unique,\n",
    "        \"tied max classes\": tied_max_classes,\n",
    "        \"duplicate ID\": duplicated_id,\n",
    "    })\n",
    "\n",
    "    if issue_mask.any():\n",
    "        n_issues = int(issue_mask.sum())\n",
    "        print(f\"Rows with potential issues: {n_issues} of {len(df)}\")\n",
    "        flagged = issue_report.loc[issue_mask].copy()\n",
    "\n",
    "        bool_cols = [\n",
    "            \"missing_or_non_numeric\",\n",
    "            \"negative_probability\",\n",
    "            \"probability_gt_1\",\n",
    "            \"row_sum_not_1\",\n",
    "            \"max_not_unique\",\n",
    "            \"duplicate_id\",\n",
    "        ]\n",
    "        for col in bool_cols:\n",
    "            flagged[col] = flagged[col].map(lambda value: \"⚠\" if bool(value) else \"\")\n",
    "\n",
    "        flagged[\"row_sum\"] = flagged[\"row_sum\"].map(lambda value: f\"{float(value):.3f}\")\n",
    "        flagged[\"tied_max_classes\"] = flagged[\"tied_max_classes\"].replace(\"\", \"—\")\n",
    "\n",
    "        flagged = flagged.rename(columns={\n",
    "            \"row_position\": \"Row position\",\n",
    "            \"row_sum\": \"Row sum\",\n",
    "            \"missing_or_non_numeric\": \"Missing/non-numeric\",\n",
    "            \"negative_probability\": \"Negative probability\",\n",
    "            \"probability_gt_1\": \"Probability > 1\",\n",
    "            \"row_sum_not_1\": \"Row sum not 1\",\n",
    "            \"max_not_unique\": \"Max not unique\",\n",
    "            \"tied_max_classes\": \"Tied max classes\",\n",
    "            \"duplicate_id\": \"Duplicate ID\",\n",
    "        })\n",
    "\n",
    "        if id_col in df.columns:\n",
    "            flagged = flagged.set_index(\"id\")\n",
    "            flagged.index.name = \"ID\"\n",
    "            display(flagged)\n",
    "        else:\n",
    "            flagged = flagged.set_index(\"Row position\")\n",
    "            display(flagged)\n",
    "    else:\n",
    "        print(\"No row-level issues found under current checks.\")\n",
    "\n",
    "# Shared class columns already defined in previous cell as class_cols_same\n",
    "probability_table_report(prob_map_same, \"tests/map_data_table.csv\", class_cols_same)\n",
    "probability_table_report(prob_ref_same, \"tests/ref_data_table.csv\", class_cols_same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to read the next outputs\n",
    "\n",
    "- The next cell creates two assessment objects from the **same probabilistic inputs**: `gue_same` and `mcem_same`.\n",
    "- Printing an object shows a summary of key metrics for that method.\n",
    "- GUE reports analytical estimates (with standard errors).\n",
    "- MCEM reports simulation-based estimates (with percentile intervals).\n",
    "\n",
    "You can compare their outputs directly because they are using the same input tables and strata totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Fuzzy / probabilistic assessments on the same shared inputs\n",
    "gue_same = GUE(\n",
    "    map_data=prob_map_same,\n",
    "    ref_data=prob_ref_same,\n",
    "    strata_col=\"strata\",\n",
    "    id_col=\"id\",\n",
    "    strata_population=strata_population_same,\n",
    ")\n",
    "\n",
    "mcem_same = MCEM(\n",
    "    map_data=prob_map_same,\n",
    "    ref_data=prob_ref_same,\n",
    "    strata_col=\"strata\",\n",
    "    id_col=\"id\",\n",
    "    strata_population=strata_population_same,\n",
    "    n_simulations=3000,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Show both constructed assessment objects explicitly (labeled)\n",
    "print(\"Constructed object: gue_same\")\n",
    "display(gue_same)\n",
    "\n",
    "print(\"\\nConstructed object: mcem_same\")\n",
    "display(mcem_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Compare key metrics from GUE and MCEM on the same probabilistic data (formatted output)\n",
    "def _format_point_se(result, decimals=4, zero_tol=1e-12):\n",
    "    value, uncertainty = result\n",
    "    value = float(value)\n",
    "    if isinstance(uncertainty, tuple) and len(uncertainty) == 2:\n",
    "        lo, hi = float(uncertainty[0]), float(uncertainty[1])\n",
    "        return f\"{value:.{decimals}f} (95% CI: {lo:.{decimals}f}, {hi:.{decimals}f})\"\n",
    "    se = float(uncertainty)\n",
    "    if abs(se) < zero_tol:\n",
    "        se = 0.0\n",
    "    return f\"{value:.{decimals}f} ± {se:.{decimals}f}\"\n",
    "\n",
    "print(\n",
    "    \"Target class:\",\n",
    "    target_class_same,\n",
    "    \"(the first class column detected in the map/reference probability files)\",\n",
    ")\n",
    "\n",
    "print(\"\\n=== GUE (Analytical on probabilistic inputs) ===\")\n",
    "print(\"Overall Accuracy:\", _format_point_se(gue_same.overall_accuracy()))\n",
    "print(\n",
    "    f\"Reference Area ({target_class_same}):\",\n",
    "    _format_point_se(gue_same.area(target_class_same, reference=True), decimals=2),\n",
    ")\n",
    "\n",
    "print(\"\\n=== MCEM (Simulation on probabilistic inputs) ===\")\n",
    "print(\"Overall Accuracy:\", _format_point_se(mcem_same.overall_accuracy()))\n",
    "print(\n",
    "    f\"Reference Area ({target_class_same}):\",\n",
    "    _format_point_se(mcem_same.area(target_class_same, reference=True), decimals=2),\n",
    ")\n",
    "\n",
    "print(\"\\nTip: GUE uses analytical SEs; MCEM uses simulation percentile intervals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Convert the SAME probabilistic inputs to crisp labels (argmax per row)\n",
    "map_crisp_labels = prob_map_same[class_cols_same].idxmax(axis=1)\n",
    "ref_crisp_labels = prob_ref_same[class_cols_same].idxmax(axis=1)\n",
    "\n",
    "# Build one hardened table used by both crisp methods\n",
    "crisp_same = pd.DataFrame({\n",
    "    \"id\": prob_map_same[\"id\"],\n",
    "    \"Map class\": map_crisp_labels,\n",
    "    \"Reference class\": ref_crisp_labels,\n",
    "})\n",
    "\n",
    "# Build one shared class-population dictionary from hardened map frequencies\n",
    "mapped_props_same = crisp_same[\"Map class\"].value_counts(normalize=True)\n",
    "mapped_population_same = {k: float(v * N_same) for k, v in mapped_props_same.items()}\n",
    "\n",
    "# For a direct comparison, configure Stehman with map class as strata\n",
    "crisp_same_stehman = crisp_same.copy()\n",
    "crisp_same_stehman[\"strata\"] = crisp_same_stehman[\"Map class\"]\n",
    "\n",
    "# Construct both crisp assessment objects from the same hardened data assumptions\n",
    "stehman_same = Stehman(\n",
    "    data=crisp_same_stehman,\n",
    "    strata_col=\"strata\",\n",
    "    map_col=\"Map class\",\n",
    "    ref_col=\"Reference class\",\n",
    "    strata_population=mapped_population_same,\n",
    ")\n",
    "\n",
    "olofsson_same = Olofsson(\n",
    "    crisp_same,\n",
    "    mapped_population_same,\n",
    "    map_col=\"Map class\",\n",
    "    ref_col=\"Reference class\",\n",
    ")\n",
    "\n",
    "print(\"Hardened table used for both crisp methods (head, indexed by id):\")\n",
    "display(crisp_same.set_index(\"id\").head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Crisp assessments from the same hardened table and shared class populations\n",
    "print(\"=== STEHMAN (Crisp, shared assumptions) ===\")\n",
    "print(\"Overall Accuracy:\", stehman_same.overall_accuracy())\n",
    "print(f\"Reference Area({target_class_same}):\", stehman_same.area(target_class_same, reference=True))\n",
    "\n",
    "print(\"\\n=== OLOFSSON (Crisp, shared assumptions) ===\")\n",
    "print(\"Overall Accuracy:\", olofsson_same.overall_accuracy())\n",
    "print(f\"Reference Area({target_class_same}):\", olofsson_same.area(target_class_same))\n",
    "\n",
    "print(\"\\nTip: if assumptions match, point estimates should be close; uncertainty terms may still differ by method.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6VE_pZeSefM"
   },
   "source": [
    "# Stehman Assessment Example\n",
    "\n",
    "The Stehman assessment is based on:\n",
    "Stehman, S.V., 2014. \"Estimating area and map accuracy for stratified\n",
    "random sampling when the strata are different from the map classes\",\n",
    "International Journal of Remote Sensing. Vol. 35 (No. 13).\n",
    "https://doi.org/10.1080/01431161.2014.930207"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGq4O5gmS1Mp"
   },
   "source": [
    "## Load Data\n",
    "\n",
    "Use the data from Table 2 in Stehman 2014 as the example assessment. Each row represents one pixel. The table must have at least three columns: one for the stratum each pixel was sampled from, one for the map class of each pixel and one for the reference class of each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKuXwKHLUqM_"
   },
   "outputs": [],
   "source": [
    "stehman_df = pd.read_csv(\"./tests/stehman2014_table2.csv\", skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHG9aNdcV8yH"
   },
   "source": [
    "In addition to the table of reference data, the Stehman assessment also needs a dictionary containing the total size of each of the strata. Keys in the dictionary should match the labels used in the strata column of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcNdzyzQVEPa"
   },
   "outputs": [],
   "source": [
    "stehman_strata_populations= {1: 40000, 2: 30000, 3: 20000, 4: 10000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2iLOTZGS2nw"
   },
   "source": [
    "## Create the Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YNJFJhfVqVm"
   },
   "outputs": [],
   "source": [
    "stehman_assessment = Stehman(\n",
    "    data=stehman_df,\n",
    "    strata_col=\"Stratum\",\n",
    "    map_col=\"Map class\",\n",
    "    ref_col=\"Reference class\",\n",
    "    strata_population=stehman_strata_populations\n",
    ")\n",
    "stehman_assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJ2JFS6nWPTZ"
   },
   "source": [
    "The Stehman assessment object has the same `user_accuracy`, `producers_accuracy` methods as the Cardille assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uS0US9oSle3"
   },
   "source": [
    "# Olofsson Assessment Example\n",
    "\n",
    "The Olofsson assessment is based on: Olofsson, P., et al., 2014 \"Good practices for estimating area and assessing accuracy of land change\", Remote Sensing of Environment. Vol 148 pp. 42-57 https://doi.org/10.1016/j.rse.2014.02.015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWGrQOIXS7z5"
   },
   "source": [
    "## Load Data\n",
    "\n",
    "Use the data from Table 8 in Olofsson et al. 2014 for the example.\n",
    "\n",
    "The Olofsson assessment can either be initialized with an error matrix of pixel counts plus a dictionary of the mapped areas or with a longform table of each pixels map and reference values. Both produce the same results, pick the one that matches the form that your data is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTpb98WEYWUb"
   },
   "outputs": [],
   "source": [
    "olofsson_mapped_populations = {\n",
    "    \"Deforestation\": 200000,\n",
    "    \"Forest gain\": 150000,\n",
    "    \"Stable forest\": 3200000,\n",
    "    \"Stable non-forest\": 6450000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHsQ-O9sWi8g"
   },
   "outputs": [],
   "source": [
    "# from an error matrix\n",
    "olofsson_data = [\n",
    "    [66, 0, 1, 2],\n",
    "    [0, 55, 0, 1],\n",
    "    [5, 8, 153, 9],\n",
    "    [4, 12, 11, 313],\n",
    "]\n",
    "classes = [\"Deforestation\", \"Forest gain\", \"Stable forest\", \"Stable non-forest\"]\n",
    "olofsson_error_matrix = pd.DataFrame(olofsson_data, index=classes, columns=classes)\n",
    "\n",
    "olofsson_error_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4X412qoeXiM2"
   },
   "outputs": [],
   "source": [
    "olofsson_assessment1 = Olofsson(olofsson_error_matrix, olofsson_mapped_populations)\n",
    "olofsson_assessment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8mLN9hgXoqS"
   },
   "outputs": [],
   "source": [
    "# from a lonfrom table\n",
    "# first convert the error matrix to the long form\n",
    "from acc_assessment.utils import _expand_error_matrix\n",
    "olofsson_longform = _expand_error_matrix(olofsson_error_matrix, \"map\", \"ref\")\n",
    "olofsson_longform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuZsb5Q0X69P"
   },
   "outputs": [],
   "source": [
    "# to tell it that you are passing a longform table tell it the names of the map\n",
    "# and the reference columns\n",
    "olofsson_assessment2 = Olofsson(\n",
    "    olofsson_longform,\n",
    "    olofsson_mapped_populations,\n",
    "    map_col=\"map\",\n",
    "    ref_col=\"ref\",\n",
    ")\n",
    "olofsson_assessment2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0h2BhcarTBZ3"
   },
   "source": [
    "## Create the Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "# GUE Assessment Example\n",
    "\n",
    "Use probabilistic map and reference tables to compute analytical estimates with GUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "map_table = pd.read_csv(\"./tests/map_data_table.csv\")\n",
    "ref_table = pd.read_csv(\"./tests/ref_data_table.csv\")\n",
    "strata_population_dict = {\n",
    "    \"a\": 5000,\n",
    "    \"f\": 10000,\n",
    "    \"w\": 1000,\n",
    "}\n",
    "\n",
    "gue_assessment = GUE(\n",
    "    map_data=map_table,\n",
    "    ref_data=ref_table,\n",
    "    strata_col=\"strata\",\n",
    "    id_col=\"id\",\n",
    "    strata_population=strata_population_dict,\n",
    ")\n",
    "gue_assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "# MCEM Assessment Example\n",
    "\n",
    "Run Monte Carlo simulations using the same probabilistic inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "mcem_assessment = MCEM(\n",
    "    map_data=map_table,\n",
    "    ref_data=ref_table,\n",
    "    strata_col=\"strata\",\n",
    "    id_col=\"id\",\n",
    "    strata_population=strata_population_dict,\n",
    "    n_simulations=10000,\n",
    ")\n",
    "mcem_assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "# Comparing GUE to MCEM\n",
    "The mean of the MCEM simulation distribution should converge to the GUE point estimate. While point estimates (Accuracy and Area) should align, MCEM percentile-based confidence intervals can differ from GUE analytical standard errors, providing a more honest representation of uncertainty for rare classes like deforestation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "print(\"=== ANALYTICAL RESULTS (GUE) ===\")\n",
    "print(gue_assessment)\n",
    "\n",
    "print(\"\\n=== MONTE CARLO RESULTS (MCEM) ===\")\n",
    "print(mcem_assessment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eecFY-NtSpqJ"
   },
   "source": [
    "# Congalton Assessment Example\n",
    "\n",
    "The Congalton assessment is based on: \"A Review of Assessing the Accuracy of Classifications of Remotely Sensed Data\", Congalton, R. G., 1991. Remote Sensing of Environment, Vol. 37. pp 35-46 https://doi.org/10.1016/0034-4257(91)90048-B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVkpSuCgTHng"
   },
   "source": [
    "## Load Data\n",
    "\n",
    "Use the data from Table 1 in Congalton 1991 to create the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OkisCJPTOR0"
   },
   "outputs": [],
   "source": [
    "congalton_data = [[65, 4, 22, 24], [6, 81, 5, 8], [0, 11, 85, 19], [4, 7, 3, 90]]\n",
    "congalton_classes = [\"D\", \"C\", \"BA\", \"SB\"]\n",
    "congalton_df = pd.DataFrame(congalton_data, index=congalton_classes, columns=congalton_classes)\n",
    "congalton_table = pd.DataFrame(_expand_error_matrix(congalton_df, \"map\", \"ref\"),)\n",
    "congalton_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXbIT5MyTJ2b"
   },
   "source": [
    "## Create the Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9i1NOjsajEk"
   },
   "outputs": [],
   "source": [
    "congalton_assessment = Congalton(congalton_table, \"map\", \"ref\")\n",
    "congalton_assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFxh2ddz5J9S"
   },
   "source": [
    "# Cardille Assessment Example\n",
    "\n",
    "The Cardille Assessment is based on ongoing work in the Cardille Computational Landscape Ecology Lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK5ObHhi5SAO"
   },
   "source": [
    "## Load data\n",
    "\n",
    "Data should come in two csv files: one containing the map data and one containing the reference data. Each file should have one column containing the point id (to link rows from the map csv file to the reference csv file), and one column for the strata that the point was sampled from, and then one column for each of the possible classes containing the reference/map probability that the point belongs to that class. Column names should match between the two csv files.\n",
    "\n",
    "In addition to the two csv files you also need to supply a dictionary mapping the stratum to their total size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wAVv7105Otu"
   },
   "outputs": [],
   "source": [
    "map_table = pd.read_csv(\"./tests/map_data_table.csv\")\n",
    "map_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jf-PDKPB6EII"
   },
   "outputs": [],
   "source": [
    "ref_table = pd.read_csv(\"./tests/ref_data_table.csv\")\n",
    "ref_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pmh6NXj-odi"
   },
   "outputs": [],
   "source": [
    "strata_population_dict = {\n",
    "    'a': 5000,\n",
    "    'f': 10000,\n",
    "    'w': 1000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HCTVamq6L3A"
   },
   "source": [
    "## Create the assessment\n",
    "\n",
    "The Cardille accuracy assessment is a class. An explanation of its constructor can be viewed by calling `help` on the `__init__` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uj8QKkg7RQv"
   },
   "outputs": [],
   "source": [
    "help(Cardille.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwb3s4B36Sgh"
   },
   "outputs": [],
   "source": [
    "assessment = Cardille(\n",
    "    map_data=map_table,\n",
    "    ref_data=ref_table,\n",
    "    strata_col=\"strata\",\n",
    "    id_col=\"id\",\n",
    "    strata_population=strata_population_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjLGKMaL_V6j"
   },
   "source": [
    "## View assessment results\n",
    "\n",
    "An overview of the assessment can be seen by printing the assessment object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcsQs0K__tzW"
   },
   "outputs": [],
   "source": [
    "assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VtYfmcUdTv8"
   },
   "source": [
    "Individual accuracies can be accessed by calling the appropriate method.\n",
    "\n",
    "These methods all return a tuple of two floats which are the value and the standard error respectively. If the standard error is not calculable it will be returned as `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkP-8QsedRkS"
   },
   "outputs": [],
   "source": [
    "forest_users_accuracy = assessment.users_accuracy(\"Forest\")\n",
    "forest_producers_accuracy = assessment.producers_accuracy(\"Forest\")\n",
    "\n",
    "print(forest_users_accuracy)\n",
    "print(forest_producers_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jb9r3wVAd5RY"
   },
   "source": [
    "To follow the practices outlined in Olofsson 2014 and Stehman 2014, the error matrix is returned as proportion of area by default. You can get an error matrix of point counts by setting `proportions=False`. Note that for the Cardille assessment these counts are scaled by their \"weights\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5C_tfS0eQ0L"
   },
   "outputs": [],
   "source": [
    "assessment.error_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7T-a3gX8eY6E"
   },
   "outputs": [],
   "source": [
    "assessment.error_matrix(proportions=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MPt-QAleqRu"
   },
   "source": [
    "An overview of all the methods can be seen by calling `help`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klkJOHdXem_M"
   },
   "outputs": [],
   "source": [
    "help(assessment)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "AccuracyAssessmentTools-hfff.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
